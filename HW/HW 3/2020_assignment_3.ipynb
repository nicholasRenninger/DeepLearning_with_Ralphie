{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"2020_assignment_3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"pagedown,pageup,pagedown,pageup,pagedown,pageup,pagedown,pageup"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"oldHeight":669,"position":{"height":"40px","left":"1221px","right":"20px","top":"257px","width":"350px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"varInspector_section_display":"none","window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Kyfqk1ZwV3we"},"source":["# Recurrent Neural Network\n","***\n","**Name**: Nicholas Renninger\n","***"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x2HnzA-ZURSu"},"source":["## Goal\n","The goal of this assignment is to use TensorFlow to build some recurrent neural nets (RNNs) and to understand their limitations through experimentation.\n","\n","\n","## The Task\n","You will implement a recurrent neural net to learn the parity operator. The net will have a single input unit and a single output unit, and a fully-connected layer of H hidden units. The inputs and target outputs are binary. When an input sequence is presented, the output state at the end of the sequence should be a parity bit: output should be 1 if the input has an odd number of '1' values. For example, the sequence 1-0-0-1-0-1 should yield output 1 and the sequence 0-0-0-0-1-1 should yield output 0. Note that a target is given only at the end of each sequence. (Parity is easy to learn if there is a target at each step that indicates parity given the sequence so far.)\n","\n","Parity is a hard problem for neural nets to learn because very similar inputs produce different outputs, and very dissimilar inputs can produce the same output.\n","\n","The aspects of the task we will manipulate are:  H, the number of hidden units, N, the length of the input strings, and the activation function for the hidden units, either tanh or LSTM-style neurons. The output neuron should have a logistic activation function.\n","\n","Tip: get started early.  Depending on your system, these nets can take several minutes to train.  Exploring hyperparameters (such as training rate) will be critical for success.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kKEl35tibl8t"},"source":["### Some Help\n","Below are some helper codes to:\n","- generate input strings and their parity.\n","- provide a callback to trigger early stopping during training\n","- plotting"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T00:42:15.000437Z","start_time":"2020-03-02T00:42:14.993356Z"},"code_folding":[],"colab_type":"code","id":"Rsf2q8L6UY8n","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"69aeef1d-ce00-4cc2-81b3-7a75d65f4a74","executionInfo":{"status":"ok","timestamp":1583180629588,"user_tz":420,"elapsed":3774,"user":{"displayName":"Nicholas Renninger","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK-DBi8Xtg33cAQvSTRW-SjYXWdB8OLW-lS4GREw=s64","userId":"02710885383179519779"}}},"source":["# %load_ext tensorboard\n","%config InlineBackend.figure_format = 'retina'\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from datetime import datetime\n","import os\n","import logging\n","\n","# don't want to try to use TPU when not in colab\n","from contextlib import suppress\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    %tensorflow_version 2.x\n","except:\n","    IN_COLAB = False\n","    from contextlib import nullcontext\n","\n","import tensorflow as tf\n","from tensorflow.keras import models\n","from tensorflow.keras import layers\n","from tensorflow.keras import optimizers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_MdEWmY0kg5P"},"source":["Resolve TPU Address"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T00:42:16.304053Z","start_time":"2020-03-02T00:42:16.301212Z"},"code_folding":[0],"colab_type":"code","id":"LlR15g2EkdRJ","colab":{}},"source":["WANT_TO_USE_TPU = False\n","USE_TPU = IN_COLAB and WANT_TO_USE_TPU\n","if USE_TPU:\n","    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n","        tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","    tf.config.experimental_connect_to_cluster(resolver)\n","    tf.tpu.experimental.initialize_tpu_system(resolver)\n","\n","    logger = tf.get_logger()\n","    logger.setLevel(logging.WARN)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T00:42:16.970158Z","start_time":"2020-03-02T00:42:16.960241Z"},"code_folding":[0,6,12,50],"colab_type":"code","id":"mgumykezkVoc","colab":{}},"source":["def gen_binary_sequences(num, length):\n","    '''\n","    Generate :num: sequences of length :length:.\n","    '''\n","    return np.float32(np.random.randint(0, 2, size=(num, length, 1), dtype=np.int32))\n","\n","def calc_parity(seqs):\n","    '''\n","    Calculate sequence parity (1 if odd number of 1s, 0 if even number of 1s)\n","    '''\n","    return np.float32(seqs.sum(axis=1) % 2)\n","\n","def plot_accuracies(accuracies, Ns, Hs, plot_title):\n","    '''\n","    Make a graph of mean % correct (and standard error) on the test set for the\n","    different values of H and N.\n","\n","    Input: ndarray of |Hs|x|Ns|x(reps)\n","         actual values of Ns and Hs\n","    '''\n","    \n","    # make it wide to see the timeseries more easily\n","    plt.rcParams['figure.figsize'] = [15, 5]\n","    \n","    lenH,lenN,lenreps = accuracies.shape\n","    assert(lenH == len(Hs))\n","    assert(lenN == len(Ns))\n","    \n","    accuracies_mean = accuracies.mean(axis=2)\n","    accuracies_std = accuracies.std(axis=2)\n","    accuracies_stderr = accuracies_std/np.sqrt(lenreps)\n","\n","    # plot\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    ax.axhline(0.5,linestyle=\"--\",color=\"gray\") # chance baseline\n","    centers = np.arange(lenN)\n","\n","    for Hindex, H in enumerate(Hs):\n","        ax.bar(centers + 0.8/lenH*(Hindex-(lenH-1)/2),\n","               accuracies_mean[Hindex], 0.8/lenH,\n","               yerr=accuracies_stderr[Hindex],\n","               alpha=0.5, label=f\"{H} hidden units\")\n","\n","    ax.set_xlabel(\"sequence length\")\n","    plt.xticks(centers,Ns)\n","    ax.set_ylabel(\"test accuracy\")\n","    plt.legend(loc=\"lower left\")\n","    plt.title(plot_title)\n","\n","class create_accuracy_callback(tf.keras.callbacks.Callback):\n","    '''\n","    Callback function to stop training at a minimum accuracy\n","    '''\n","    def on_epoch_end(self, epoch, logs={}, min_accuracy=0.98):\n","\n","        if(logs['accuracy'] >= min_accuracy and\n","           logs['val_accuracy'] >= min_accuracy):\n","            self.model.stop_training = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fyhd-HYBdcF0"},"source":["## Part 1\n","\n","Experiments with vanilla RNNs $\\textrm{tanh}$ activations."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"owDFuSt6hSE5"},"source":["### Part 1.a\n","Fill in the code to create a net given H and N using the tanh activation function for the hidden units.  Keras has a number of RNN helper functions, although you can also write your own custom layers.\n","\n","Remember that the net should only take one bit of input at a time from the input sequence, and output one logistic value (between 0 and 1) only after the input sequence is complete."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T01:26:26.670664Z","start_time":"2020-03-02T01:26:26.665870Z"},"code_folding":[0],"colab_type":"code","id":"AJuSiefzejFZ","colab":{}},"source":["def build_tanh_model(N, H):\n","    '''\n","    Builder for an RNN model with tanh hidden activations.\n","    Model inputs are binary sequences of length :N:.\n","\n","    At each sequence position, the input and prior state should be fully\n","    connected to :H: hidden units with tanh activation.\n","\n","    The output of the last state of the RNN should be fully connected to a \n","    single unit with logistic activation, to perform the final \n","    classification of the sequence.\n","    '''\n","\n","    # dropout is bad lol\n","    model = models.Sequential([\n","        layers.Bidirectional(layers.SimpleRNN(units=H,\n","                                              activation='tanh',\n","                                              dropout=0.0,\n","                                              recurrent_dropout=0.0)),\n","        layers.Dense(1, activation='sigmoid')\n","        ], name='tanh')\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k0O0WxWqnTD7"},"source":["### Part 1.b\n","Then fill in the code to train several such nets.  Each repetition should randomize the initial weights and generate a random training set of 10000 examples of length N as well as a random test set of 10000 examples of length N.  Save 10% of the training as validation, and use at least the provided check_accuracy callback as an early stopping condition.\n","\n","Train nets for H ∈ {5, 25} and for N ∈ {5, 10, 15, 20}.  For each combination of H and N, run 10 replications of your simulation.  You will also need to try to find helpful learning rates; don't be surprised if your training is prone to cycles of stagnation for hundreds of epochs before quickly learning."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T01:34:54.106195Z","start_time":"2020-03-02T01:34:54.101593Z"},"code_folding":[],"colab_type":"code","id":"wpOs5URgWsQK","run_control":{"marked":true},"colab":{}},"source":["def train_model(model, train_dataset, epochs, val_dataset,\n","                early_termination_callback, alpha, momentum):\n","    '''\n","    Trains the :model:.\n","    \n","    return:\n","    \n","    test_accuracy (float): a list the length of epochs that contains the\n","                           model's val_acc metric during each epoch of training\n","    '''\n","\n","    # start by compiling the model; accuracy should be a tracked metric\n","\n","    if model.name == 'LSTM':\n","        opt = optimizers.RMSprop(learning_rate=alpha, momentum=momentum)\n","    elif model.name == 'tanh':\n","        opt = optimizers.Adamax(lr=alpha)\n","    else:\n","        raise ValueError(f'model.name={model.name} must be \"LSTM\" or \"tanh\"')\n","\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer=opt,\n","                  metrics=['accuracy'])\n","\n","    # train model\n","    history = model.fit(train_dataset,\n","                        epochs=epochs,\n","                        verbose=0,\n","                        validation_data=val_dataset,\n","                        callbacks=[early_termination_callback])\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7KHP6B7JhSFC"},"source":["Now, we need to define a function to take all of our model / training parameter exploration ranges and:\n","\n","* perform multiple experiments per parameter combination\n","* train the desired RNN model using different combination parameters for each experiment \n","* collect and return the test set accuracy the model using each set of parameter pairs for each experiment"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T01:26:29.237127Z","start_time":"2020-03-02T01:26:29.224918Z"},"code_folding":[0],"colab_type":"code","id":"jmLQlzRNfiUJ","colab":{}},"source":["def study_rnn_performance(model_builder, Hs, Ns, early_termination_callback,\n","                          alphas, momentums,\n","                          REPS=10, SAMPLES=10000, BATCH_SIZE=1000,\n","                          MAX_EPOCHS=2000):\n","\n","    test_accs = np.zeros([len(Hs), len(Ns), REPS])\n","\n","    for ((Nindex, N), alpha, momentum) in zip(enumerate(Ns), alphas, momentums):\n","        print('')\n","        for rep in range(REPS):\n","\n","            # (data is reusable across changes to the number of hidden units)\n","            # generate a random training set of SAMPLES examples of length N\n","            x = gen_binary_sequences(num=(2 * SAMPLES), length=N)\n","            y = calc_parity(x)\n","\n","            (x_train, x_test,\n","             y_train, y_test) = train_test_split(x, y, test_size=SAMPLES)\n","\n","            (x_train, x_val,\n","             y_train, y_val) = train_test_split(x_train, y_train,\n","                                                test_size=0.1)\n","\n","            # convert to tf datasets for speed\n","            train_dataset = tf.data.Dataset.from_tensor_slices((x_train,\n","                                                                y_train))\n","            test_dataset = tf.data.Dataset.from_tensor_slices((x_test,\n","                                                               y_test))\n","            val_dataset = tf.data.Dataset.from_tensor_slices((x_val,\n","                                                              y_val))\n","\n","            # use tf to apply the batch size operations\n","            train_dataset = train_dataset.batch(BATCH_SIZE)\n","            test_dataset = test_dataset.batch(BATCH_SIZE)\n","            val_dataset = val_dataset.batch(BATCH_SIZE)\n","\n","            # cache the whole, small datasets in wham\n","            train_dataset = train_dataset.cache()\n","            test_dataset = test_dataset.cache()\n","            val_dataset = val_dataset.cache()\n","\n","            for Hindex, H in enumerate(Hs):\n","                print(f\"starting N={N}, H={H}, rep {rep + 1} / {REPS}\")\n","\n","                if USE_TPU:\n","                    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n","                else:\n","                    strategy = None\n","\n","                with (strategy.scope() if USE_TPU else suppress()):\n","\n","                    model = model_builder(N, H)\n","\n","                    model = train_model(model, train_dataset,\n","                        epochs=MAX_EPOCHS,\n","                        val_dataset=val_dataset,\n","                        early_termination_callback=early_termination_callback,\n","                        alpha=alpha, momentum=momentum)\n","                    \n","                    results = model.evaluate(test_dataset)\n","                    test_acc = results[1]\n","                    \n","                    # save for plotting later\n","                    test_accs[Hindex, Nindex, rep] = test_acc\n","                    \n","                    print(f'test accuracy: {test_acc}')   \n","                \n","    return test_accs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n9DHl1lEhSFG"},"source":["For this part, we will explore a range of sequence lengths and number of hidden units in an using tanh hidden layer activations RNN:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T01:33:40.138354Z","start_time":"2020-03-02T01:26:47.139729Z"},"colab_type":"code","executionInfo":{"status":"ok","timestamp":1583144361755,"user_tz":420,"elapsed":9322681,"user":{"displayName":"Nicholas Renninger","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjK-DBi8Xtg33cAQvSTRW-SjYXWdB8OLW-lS4GREw=s64","userId":"02710885383179519779"}},"id":"ZBA3oCkshSFH","outputId":"3a1c9066-86b0-4a3d-b330-ca798143f2de","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":986}},"source":["Hs = [5, 25]\n","Ns = [5, 10, 20, 40] # 1e-1, 5e-3, 5e-3, who Cares\n","check_accuracy = create_accuracy_callback()\n","\n","# Adamax\n","alphas = [1e-1, 5e-3, 3e-3, 1e-1]\n","momentums = [None, None, None, None]\n","\n","test_accs = study_rnn_performance(build_tanh_model, Hs, Ns,\n","                                  alphas=alphas, momentums=momentums,\n","                                  early_termination_callback=check_accuracy)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","starting N=5, H=5, rep 1 / 10\n","10/10 [==============================] - 0s 5ms/step - loss: 0.0679 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=25, rep 1 / 10\n","10/10 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=5, rep 2 / 10\n","10/10 [==============================] - 0s 5ms/step - loss: 0.1130 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=25, rep 2 / 10\n","10/10 [==============================] - 0s 4ms/step - loss: 0.1287 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=5, rep 3 / 10\n","10/10 [==============================] - 0s 5ms/step - loss: 0.0684 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=25, rep 3 / 10\n","10/10 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=5, rep 4 / 10\n","10/10 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=25, rep 4 / 10\n","10/10 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=5, rep 5 / 10\n","10/10 [==============================] - 0s 5ms/step - loss: 0.0589 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=25, rep 5 / 10\n","10/10 [==============================] - 0s 4ms/step - loss: 0.1493 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=5, rep 6 / 10\n","10/10 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=25, rep 6 / 10\n","10/10 [==============================] - 0s 4ms/step - loss: 0.2146 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=5, rep 7 / 10\n","10/10 [==============================] - 0s 5ms/step - loss: 0.0725 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=25, rep 7 / 10\n","10/10 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=5, rep 8 / 10\n","10/10 [==============================] - 0s 5ms/step - loss: 0.1184 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=25, rep 8 / 10\n","10/10 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=5, rep 9 / 10\n","10/10 [==============================] - 0s 6ms/step - loss: 0.0719 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=25, rep 9 / 10\n","10/10 [==============================] - 0s 4ms/step - loss: 0.2019 - accuracy: 1.0000\n","test accuracy: 1.0\n","starting N=5, H=5, rep 10 / 10\n"," 1/10 [==>...........................] - ETA: 0s - loss: 0.1896 - accuracy: 1.0000"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u4RbGJEbexGV"},"source":["**Part 1.c**<br>\n","Make the graph of mean % correct (and standard error) on the test set for the different values of H and N."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T01:33:40.280933Z","start_time":"2020-03-02T01:33:30.822Z"},"colab_type":"code","id":"_ew6HWSpkpeW","colab":{}},"source":["plot_accuracies(test_accs, Ns, Hs,\n","                plot_title='Basic Tanh RNN Performance Exploration')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ku9WKNAafXnj"},"source":["## Part 2\n","Repeat the experiments of Part 1, but use LSTM neurons instead of tanh neurons in the recurrent layer.  Comment on your experiences."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GU7uynCutpJ7"},"source":["To do this part, all we will need to do is write a new model builder using an LSTM recurrent layer:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T01:47:26.460978Z","start_time":"2020-03-02T01:47:26.457941Z"},"code_folding":[],"id":"fGjx4Sjq8XO1","colab_type":"code","colab":{}},"source":["def build_LSTM_model(N, H):\n","    '''\n","    Builder for an RNN model with LSTM hidden neurons.\n","    Model inputs are binary sequences of length :N:.\n","\n","    At each sequence position, the input and prior state should be fully\n","    connected to :H: LSTM hidden units neurons.\n","\n","    The output of the last state of the RNN should be fully connected to a \n","    single unit with logistic activation, to perform the final \n","    classification of the sequence.\n","    '''\n","\n","    # dropout is bad lol\n","    model = models.Sequential([\n","        layers.Bidirectional(layers.LSTM(units=H,\n","                                         dropout=0.0)),\n","        layers.Dense(1, activation='sigmoid')\n","        ], name=\"LSTM\")\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OCmtqHr48XO3","colab_type":"text"},"source":["Now, we will re-use the training and analysis functions from Part 1 to compare the LSTM performance:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-03-02T01:51:40.911564Z","start_time":"2020-03-02T01:49:09.741292Z"},"scrolled":true,"id":"1AMUvMZL8XO4","colab_type":"code","colab":{}},"source":["Hs = [5, 25]\n","Ns = [5, 10, 20, 40] # 9e-3, 9e-3, 4e-3, who Cares\n","check_accuracy = create_accuracy_callback()\n","\n","# RMSProp\n","alphas = [9e-3, 9e-3, 4e-3, 1e-3]\n","momentums = [1e-4, 1e-4, 2e-4, 2e-4]\n","\n","test_accs = study_rnn_performance(build_LSTM_model, Hs, Ns,\n","                                  alphas=alphas, momentums=momentums,\n","                                  early_termination_callback=check_accuracy)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5P3b9pJm5NlN","colab_type":"code","colab":{}},"source":["plot_accuracies(test_accs, Ns, Hs,\n","                plot_title='LSTM Performance Exploration')"],"execution_count":0,"outputs":[]}]}