{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Implemented FF Neural Networks\n",
    "***\n",
    "**Name**: Nicholas Renninger\n",
    "***\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this assignment is to build a one-hidden-layer back propagation network to process real data.  For this assignment you will implement the neural net (activation function and training code) yourself, not using tensorflow or other deep learning frameworks. The purpose is for you to understand the nitty gritty of what these frameworks are doing for you before we switch over to using them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "***\n",
    "\n",
    "In this assignment you will be using the Occupancy Detection data set (ODD). It consists of experimental data used for binary classification of room occupancy (i.e., room is occupied versus empty) based on temperature, humidity, light, and CO2 sensors. The train and test data sets are each collected over a week period.\n",
    "\n",
    "The data set includes time stamps with date and hour/minute/second within the day. You are **not to use time stamp features** for predicting occupancy. Since this is a commercial office building, the time stamp is a strong predictor of occupancy. Rather, the goal is to determine whether occupancy - **occ** can be sensed from:\n",
    "1. temperature, expressed in degrees Celsius - **temp**\n",
    "2. relative humidity, expressed as a % - **hum**\n",
    "3. light, in lux - **light**\n",
    "4. CO2, in ppm - **cdx**\n",
    "5. humidity ratio, which is derived from the temperature and the relative humidity - **hum_ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:37:20.153457Z",
     "start_time": "2020-02-19T22:37:19.065018Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import numpy.linalg\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import normalize\n",
    "import copy\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:37:20.185428Z",
     "start_time": "2020-02-19T22:37:20.154872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-04 17:51:00</td>\n",
       "      <td>23.18</td>\n",
       "      <td>27.2720</td>\n",
       "      <td>426.0</td>\n",
       "      <td>721.250000</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-04 17:51:59</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-04 17:53:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2450</td>\n",
       "      <td>426.0</td>\n",
       "      <td>713.500000</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-04 17:54:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>708.250000</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-02-04 17:55:00</td>\n",
       "      <td>23.10</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>704.500000</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8139</th>\n",
       "      <td>2015-02-10 09:29:00</td>\n",
       "      <td>21.05</td>\n",
       "      <td>36.0975</td>\n",
       "      <td>433.0</td>\n",
       "      <td>787.250000</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8140</th>\n",
       "      <td>2015-02-10 09:29:59</td>\n",
       "      <td>21.05</td>\n",
       "      <td>35.9950</td>\n",
       "      <td>433.0</td>\n",
       "      <td>789.500000</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8141</th>\n",
       "      <td>2015-02-10 09:30:59</td>\n",
       "      <td>21.10</td>\n",
       "      <td>36.0950</td>\n",
       "      <td>433.0</td>\n",
       "      <td>798.500000</td>\n",
       "      <td>0.005596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8142</th>\n",
       "      <td>2015-02-10 09:32:00</td>\n",
       "      <td>21.10</td>\n",
       "      <td>36.2600</td>\n",
       "      <td>433.0</td>\n",
       "      <td>820.333333</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8143</th>\n",
       "      <td>2015-02-10 09:33:00</td>\n",
       "      <td>21.10</td>\n",
       "      <td>36.2000</td>\n",
       "      <td>447.0</td>\n",
       "      <td>821.000000</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8143 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  Temperature  Humidity  Light         CO2  \\\n",
       "1     2015-02-04 17:51:00        23.18   27.2720  426.0  721.250000   \n",
       "2     2015-02-04 17:51:59        23.15   27.2675  429.5  714.000000   \n",
       "3     2015-02-04 17:53:00        23.15   27.2450  426.0  713.500000   \n",
       "4     2015-02-04 17:54:00        23.15   27.2000  426.0  708.250000   \n",
       "5     2015-02-04 17:55:00        23.10   27.2000  426.0  704.500000   \n",
       "...                   ...          ...       ...    ...         ...   \n",
       "8139  2015-02-10 09:29:00        21.05   36.0975  433.0  787.250000   \n",
       "8140  2015-02-10 09:29:59        21.05   35.9950  433.0  789.500000   \n",
       "8141  2015-02-10 09:30:59        21.10   36.0950  433.0  798.500000   \n",
       "8142  2015-02-10 09:32:00        21.10   36.2600  433.0  820.333333   \n",
       "8143  2015-02-10 09:33:00        21.10   36.2000  447.0  821.000000   \n",
       "\n",
       "      HumidityRatio  Occupancy  \n",
       "1          0.004793          1  \n",
       "2          0.004783          1  \n",
       "3          0.004779          1  \n",
       "4          0.004772          1  \n",
       "5          0.004757          1  \n",
       "...             ...        ...  \n",
       "8139       0.005579          1  \n",
       "8140       0.005563          1  \n",
       "8141       0.005596          1  \n",
       "8142       0.005621          1  \n",
       "8143       0.005612          1  \n",
       "\n",
       "[8143 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('data/datatraining.txt')\n",
    "display(data)\n",
    "X_data = data[['date', 'Humidity', 'Light', 'CO2',\n",
    "               'HumidityRatio', 'Temperature']].values\n",
    "y_data = data['Occupancy'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data[:, 1:], y_data,\n",
    "                                                    test_size=0.1)\n",
    "\n",
    "# have to explicitly cast as an np.float or everything freaks out\n",
    "X_train = np.array(X_train, dtype=np.float)\n",
    "X_test = np.array(X_test, dtype=np.float)\n",
    "y_train = np.array(y_train, dtype=np.float)\n",
    "y_test = np.array(y_test, dtype=np.float)\n",
    "\n",
    "# input normalization to prevent input saturation\n",
    "normalize(X_train)\n",
    "normalize(X_test)\n",
    "\n",
    "# make y a column vector\n",
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "y_test = np.expand_dims(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Perceptron\n",
    "***\n",
    "\n",
    "Using the perceptron code you wrote for Assignment 1, train a perceptron (linear activation function with a binary threshold) using the training set. Your perceptron should have the 5 input variables described above.\n",
    "\n",
    "**Part A**: Report the training and test set performance in terms of % examples classified correctly after 100 epochs.\n",
    "\n",
    "**NOTE**: The Perceptron Update Rule is guaranteed to converge only if there is a setting of the weights that will classify the training set perfectly.  (The learning rule minimizes mistakes. When all examples are classified correctly, the weights stop changing.)  With a noisy data set like this one, the algorithm may not terminate.  Also remember that the perceptron algorithm is performing stochastic gradient descent. Thus, it will jitter around the ideal solution continually changing the weights from one iteration to the next. The weight changes will have a small effect on performance, so you'll see training set performance jitter a bit as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:37:22.866690Z",
     "start_time": "2020-02-19T22:37:20.187604Z"
    },
    "code_folding": [
     0,
     41,
     95
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 accuracy:  0.8732259825327511\n",
      "Epoch:  10 accuracy:  0.9506004366812227\n",
      "Epoch:  20 accuracy:  0.9596069868995634\n",
      "Epoch:  30 accuracy:  0.962882096069869\n",
      "Epoch:  40 accuracy:  0.9639737991266376\n",
      "Epoch:  50 accuracy:  0.9635644104803494\n",
      "Epoch:  60 accuracy:  0.9668395196506551\n",
      "Epoch:  70 accuracy:  0.9684770742358079\n",
      "Epoch:  80 accuracy:  0.9667030567685589\n",
      "Epoch:  90 accuracy:  0.9688864628820961\n",
      "Max Epochs:  100 Max Accuracy:  0.9692958515283843\n",
      "Perceptron weight vector: [[-1.61340635e+01]\n",
      " [ 2.32450000e+00]\n",
      " [ 1.91406667e+00]\n",
      " [-4.09644288e-03]\n",
      " [-6.93185510e+01]\n",
      " [-2.89000000e+00]]\n",
      "Number of incorrectly classified examples: 225\n"
     ]
    }
   ],
   "source": [
    "def perceptron_update(X, y, w, alpha):\n",
    "    \"\"\"\n",
    "    One epoch of Perceptron updates (full sweep of the dataset).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : NumPy array of features (size : no of examples X features)\n",
    "    y : Numpy array of class labels (size : no of examples X 1)\n",
    "    w : array of coefficients from the previous iteration\n",
    "    alpha : Learning rate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : Coefficients of the classifier (after updating)\n",
    "    incorrect : Incorrectly classified examples\n",
    "                (tuple of features, predicted label)\n",
    "    accuracy: ratio of correctly classified to total examples\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(y)\n",
    "    numberClassifiedRight = N\n",
    "    incorrect = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        curr_x = X[i, :].reshape((-1, 1))\n",
    "        curr_y = y[i]\n",
    "        y_predicted = np.sign(w.T @ curr_x)\n",
    "        \n",
    "        # use perceptron update rule\n",
    "        if curr_y != y_predicted:\n",
    "            \n",
    "            w = np.add(w, alpha * curr_y * curr_x)\n",
    "\n",
    "            numberClassifiedRight -= 1\n",
    "            incorrect.append((curr_x, y_predicted))\n",
    "        \n",
    "    accuracy = numberClassifiedRight / N\n",
    "\n",
    "    return w, incorrect, accuracy\n",
    "\n",
    "def perceptron(X, y, maxIter, alpha):\n",
    "    \"\"\"\n",
    "    Implements the Perceptron algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : NumPy array of features (size : no of examples X features)\n",
    "    y : Numpy array of class labels (size : no of examples X 1)\n",
    "    maxIter : The maximum number of iterations allowed \n",
    "    alpha : Learning Rate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : Coefficients of the classifier\n",
    "    incorrect : Incorrectly classified examples on termination\n",
    "    accuracyPerEpoch : array-like of best accuracy each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    # as we want to learn the bias term as well, need to augment our feature\n",
    "    # matrix\n",
    "    X_b = getFeaturesWithBias(X)\n",
    "\n",
    "    N = len(y)\n",
    "    numFeatures = X_b.shape[1]\n",
    "\n",
    "    # initialize gradient descent with weights all being 0\n",
    "    w = np.zeros((numFeatures, 1))\n",
    "    numEpochs = 0\n",
    "    accuracyPerEpoch = []\n",
    "    maxAccuracy = 0\n",
    "    best_weights = w\n",
    "    while (numEpochs < maxIter):\n",
    "\n",
    "        w, incorrect, accuracy = perceptron_update(X_b, y, w, alpha)\n",
    "        accuracyPerEpoch.append(accuracy)\n",
    "        \n",
    "        if accuracy > maxAccuracy:\n",
    "            maxAccuracy = accuracy\n",
    "            best_weights = copy.deepcopy(w)\n",
    "\n",
    "        maxAccuracy = accuracy\n",
    "        if accuracy == 1:\n",
    "            break\n",
    "        \n",
    "       # slow down printing of loss\n",
    "        if (numEpochs % 10) == 0:\n",
    "            print('Epoch: ', numEpochs, 'accuracy: ', accuracy)\n",
    "        \n",
    "        numEpochs += 1\n",
    "    \n",
    "    print('Max Epochs: ', numEpochs, 'Max Accuracy: ', maxAccuracy)\n",
    "        \n",
    "    return best_weights, incorrect, accuracyPerEpoch\n",
    "\n",
    "def getFeaturesWithBias(X):\n",
    "    \"\"\"\n",
    "    Returns the Feature Matrix with a column of ones added for the bias term\n",
    "    \n",
    "    need to add a column of ones to the feature matrix to account for the\n",
    "    bias term 'b' in our linear model:\n",
    "            y = w_1 * x_1 + w_2 * x_2 + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : NumPy array of features (size : no of examples X features)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_b : NumPy array of features with bias column added \n",
    "          (size : no of examples X (features + 1))\n",
    "          \n",
    "    Examples\n",
    "    --------\n",
    "    X = [[0.3 0.5]\n",
    "         [0.5 0.4]\n",
    "         [0.7 1.5]]\n",
    "         \n",
    "         \n",
    "    X_b = [[0.3 0.5 1.0]\n",
    "           [0.5 0.4 1.0]\n",
    "           [0.7 1.5 1.0]]\n",
    "    \"\"\"\n",
    "    \n",
    "    numFeatures, _ = np.shape(X)\n",
    "    biasFeatureMat = np.ones(shape=(numFeatures, 1))\n",
    "    X_b = np.concatenate((X, biasFeatureMat), axis=1)\n",
    "    \n",
    "    return X_b\n",
    "\n",
    "# need negative classes to be -1\n",
    "y_perceptron_train = [-1 if label == 0 else 1 for label in y_train]\n",
    "y_perceptron_test = [-1 if label == 0 else 1 for label in y_test]\n",
    "\n",
    "(weights,\n",
    " incorrect,\n",
    " accuracyPerEpoch) = perceptron(X_train, y_perceptron_train, 100,\n",
    "                                alpha=0.001)\n",
    "\n",
    "print(\"Perceptron weight vector:\", weights)\n",
    "print(\"Number of incorrectly classified examples:\", len(incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test vs. training set performances are shown below. Very good generalization :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:37:22.910963Z",
     "start_time": "2020-02-19T22:37:22.869722Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9705240174672489\n",
      "test accuracy: 0.9668711656441717\n"
     ]
    }
   ],
   "source": [
    "def classify(X, y, w):\n",
    "    \"\"\"\n",
    "    Use this function to classify examples in the test set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Test set features\n",
    "    y : Test set labels\n",
    "    w : Perceptron coefficients\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    accuracy: ratio of correctly classified to total examples\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(y)\n",
    "    numberClassifiedRight = N\n",
    "    \n",
    "    # as we want to learn the bias term as well, need to augment our feature\n",
    "    # matrix\n",
    "    X_b = getFeaturesWithBias(X)\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        curr_x = X_b[i, :].reshape((-1, 1))\n",
    "        curr_y = y[i]\n",
    "\n",
    "        y_predicted = np.sign(w.T @ curr_x)\n",
    "        if curr_y != y_predicted:\n",
    "            numberClassifiedRight -= 1\n",
    "\n",
    "    accuracy = numberClassifiedRight / N\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# testing the perceptron\n",
    "train_accuracy = classify(X_train, y_perceptron_train, weights)\n",
    "test_accuracy = classify(X_test, y_perceptron_test, weights)\n",
    "\n",
    "print('train accuracy:', train_accuracy)\n",
    "print('test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1-layer feedforward neural net\n",
    "***\n",
    "\n",
    "In this part, you will implement a 1-layer feedforward neural network from scratch as shown in the figure below.\n",
    "<img src=\"./res/mlp_ann.png\" alt=\"mlp_ann\" style=\"width:500px;\"/>\n",
    "Your tasks are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2.A**\n",
    "\n",
    "1. Implement `sigmoid(x)`: In this assignment, we will be using the sigmoid activation function as the non-linearity.\n",
    "2. Implement `forward(x)` which calculates the activations at layer 1, 2 and 3 (stored in variables `_a01`, `_a12`, `_a23` respectively) and returns the final layer activations.\n",
    "3. Implement `backward(y, y_hat)` such that it returns layer 1 and layer 2 gradients using the mean squared loss between the predicted and groundtruth vectors.\n",
    "4. Implement `update(l1_grad, l2_grad)` function which will update the weights and biases stored in `_W1, _W2 and _b1, _b2`.\n",
    "5. Implement`train(X, y, epochs)`.\n",
    "6. Implement `predict(X)` to return one-hot encoded representation of the `_a23` layer activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:37:22.971035Z",
     "start_time": "2020-02-19T22:37:22.912609Z"
    },
    "code_folding": [
     2,
     54,
     71,
     92,
     113,
     135,
     166,
     221,
     243,
     264,
     299,
     346,
     427
    ]
   },
   "outputs": [],
   "source": [
    "rndg = np.random.RandomState(seed=0)\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "        -- Multi-Layer Perceptron Neural Net.\n",
    "           Network has 1 input layer followed by a hidden layer and\n",
    "           an output layer. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_vis=5, n_hid=10, n_out=1, batch_size=7, lr=0.1):\n",
    "        \"\"\"\n",
    "            Initialize an MLP object.\n",
    "            \n",
    "            params\n",
    "            \n",
    "            n_vis (int): # of neurons in input layer.\n",
    "            n_hid (int): # of neurons in hidden layer.\n",
    "            n_out (int): # number of neurons in final output layer.\n",
    "            batch_size (int): # number of X,y instances in a mini-batch.\n",
    "            lr (double): learning rate/step size.\n",
    "\n",
    "            class variables:\n",
    "            \n",
    "            _W1, _b1 (np.ndarray): \n",
    "                Weights and biases of the input layer respectively.\n",
    "            \n",
    "            _W2, _b2 (np.ndarray):\n",
    "                Weights and biases of the hidden layer respectively.\n",
    "            \n",
    "            _a01, _a12, _a23 (np.ndarray or None):\n",
    "                Activations from input, hidden and output layer respectively. \n",
    "                \n",
    "            _l1_grad_b1, _l2_grad_b2 (np.ndarray or None):\n",
    "                Gradients of the loss w.r.t. the biases. \n",
    "        \"\"\"\n",
    "        self._n_vis = n_vis\n",
    "        self._n_hid = n_hid\n",
    "        self._n_out = n_out\n",
    "        \n",
    "        self._W1 = rndg.normal(size=(self._n_hid, self._n_vis))\n",
    "        self._b1 = np.ones((self._n_hid, 1))\n",
    "\n",
    "        self._W2 = rndg.normal(size=(self._n_out, self._n_hid))\n",
    "        self._b2 = np.ones((self._n_out, 1))\n",
    "\n",
    "        self._a01 = None\n",
    "        self._a12 = None\n",
    "        self._a23 = None\n",
    "        \n",
    "        self._l1_grad_b1 = None\n",
    "        self._l2_grad_b2 = None\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "            Sigmoid Logistic Function.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray)\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            result (np.ndarray): result = f(X) \n",
    "                where f is the sigmoid function.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        return 1.0 / (1.0 + numpy.exp(-X)) \n",
    "    \n",
    "    def sigmoid_gradient(self, X):\n",
    "        \"\"\"\n",
    "            Gradient of the Sigmoid Logistic Function.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray)\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            result (np.ndarray): result = ∇f(X) \n",
    "                where ∇f is the gradient of the sigmoid function.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # want to use element-wise (hadamard) product to ensure proper gradient\n",
    "        # calculation\n",
    "        gradient = self.sigmoid(X) * (1.0 - self.sigmoid(X))\n",
    "\n",
    "        return gradient\n",
    "    \n",
    "    def compute_MSE_loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Computes the total MSE loss for across all y-y_hat pairs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : Numpy array of output value 'y' (size : no of examples X 1)\n",
    "        y_hat : Numpy array of estimated output value \n",
    "                (size : no of examples X 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : scalar MSE loss over the whole data set\n",
    "        \"\"\"\n",
    "\n",
    "        N = len(y)\n",
    "        residuals = (y - y_hat) ** 2\n",
    "        loss = 1 / (2 * N) * np.sum(residuals)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def compute_MSE_loss_gradient(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Computes the total MSE loss gradient w.r.t. y_hat across all y-y_hat\n",
    "        pairs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : Numpy array of output value 'y' (size : no of examples X 1)\n",
    "        y_hat : Numpy array of estimated output value\n",
    "                (size : no of examples X 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss_gradient : array of MSE loss gradient over the whole data set\n",
    "        \"\"\"\n",
    "\n",
    "        N = len(y)\n",
    "        loss_gradient = (y.flatten() - y_hat.flatten())\n",
    "        loss_gradient = loss_gradient.reshape((N, 1))\n",
    "        \n",
    "        return loss_gradient\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Feed forward an input through each \n",
    "            of the layers of network.\n",
    "            \n",
    "            Store each layer activation in the class \n",
    "            variables defined in the constructor. You'll\n",
    "            need them later during backpropogation.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray): batch_size x 5 dimensional array\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            y_hat (np.ndarray): 1 x batch_size dimensional array\n",
    "                representing final layer outputs.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self._a01 = X\n",
    "\n",
    "        batch_size = len(X)\n",
    "        b1_biases = np.matlib.repmat(self._b1.T, batch_size, 1)\n",
    "        b2_biases = np.matlib.repmat(self._b2.T, batch_size, 1)\n",
    "\n",
    "        self._a12 = self.sigmoid(self._a01 @ self._W1.T + b1_biases)\n",
    "        self._a23 = self.sigmoid(self._a12 @ self._W2.T + b2_biases)\n",
    "        \n",
    "        return self._a23\n",
    "\n",
    "    def backward(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implement the backpropogation algorithm.\n",
    "        Assume mean squared loss at the output.\n",
    "        calculate and update gradients of the loss w.r.t. the biases\n",
    "\n",
    "        params:\n",
    "\n",
    "        y (np.ndarray): batch_size x 1 dimensional vector\n",
    "            of grounf truth labels.\n",
    "        y_hat (np.nd_array): 1 x batch_size dimensional vector\n",
    "            of predicted lables from forward().\n",
    "\n",
    "        return:\n",
    "\n",
    "        W1_grad (np.ndarray): n_hid x n_vis dimensional array\n",
    "            representing gradients for hidden layer. \n",
    "            (gradients of the loss w.r.t. the _W1)\n",
    "\n",
    "        W2_grad (np.ndarray): 1 x n_hid dimensional array\n",
    "            representing gradients for input layer. \n",
    "            (gradients of the loss w.r.t. the _W2)\n",
    "        \"\"\"\n",
    "\n",
    "        # gradient w.r.t. output activations\n",
    "        output_loss = self.compute_MSE_loss_gradient(y, y_hat)\n",
    "        grad_output_activ = output_loss * self.sigmoid_gradient(y_hat)\n",
    "                            \n",
    "        # gradient w.r.t. hidden layer biases\n",
    "        _, numOutputs = grad_output_activ.shape\n",
    "        self._l2_grad_b2 = np.reshape(np.mean(grad_output_activ, axis=0),\n",
    "                                      (numOutputs, 1))\n",
    "        \n",
    "        # gradient w.r.t. hidden layer weights\n",
    "        hidden_layer_values = self._a12\n",
    "        W2_grad = np.dot(grad_output_activ.T, hidden_layer_values)\n",
    "        \n",
    "        # gradient w.r.t. hidden layer values\n",
    "        grad_hidden = np.dot(self._W2.T, grad_output_activ.T)\n",
    "        \n",
    "        # gradient w.r.t. hidden layer output activations\n",
    "        grad_hidden_activ = grad_hidden * \\\n",
    "                            self.sigmoid_gradient(hidden_layer_values).T\n",
    "        \n",
    "        # gradient w.r.t. hidden layer biases\n",
    "        numHidden, _ = grad_hidden_activ.shape\n",
    "        self._l1_grad_b1 = np.reshape(np.mean(grad_hidden_activ, axis=1),\n",
    "                                      (numHidden, 1))\n",
    "        \n",
    "        # gradient w.r.t. input layer weights\n",
    "        input_layer_values = self._a01\n",
    "        W1_grad = np.dot(grad_hidden_activ, input_layer_values)\n",
    "        \n",
    "        return W1_grad, W2_grad\n",
    "        \n",
    "    def update(self, l1_grad, l2_grad):\n",
    "        \"\"\"\n",
    "            Implement the update rule for network weights and biases.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            l1_grad (np.ndarray): gradients for input layer.\n",
    "            l2_grad (np.ndarray): gradients for hidden layer.\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            none.\n",
    "        \"\"\"\n",
    "        \n",
    "        # weight matrix updates\n",
    "        self._W1 += self.lr * l1_grad\n",
    "        self._W2 += self.lr * l2_grad\n",
    "        \n",
    "        # bias vector updates\n",
    "        self._b1 += self.lr * self._l1_grad_b1\n",
    "        self._b2 += self.lr * self._l2_grad_b2\n",
    "        \n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        \"\"\"\n",
    "        Returns one hot encoding of vector X using 0.5\n",
    "        as the default threshold.\n",
    "\n",
    "        params:\n",
    "\n",
    "        X (np.ndarray): N x 5 dimensional ndarray of inputs.\n",
    "\n",
    "        return:\n",
    "\n",
    "        y (np.ndarray): one hot encoding of output layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_hat = self.forward(X)\n",
    "        y_hat_labels = [0 if value <= threshold else 1 for value in y_hat]\n",
    "        \n",
    "        y_hat_labels = np.array(y_hat_labels)\n",
    "        \n",
    "        return y_hat_labels\n",
    "    \n",
    "    def get_batch_indices(self, curr_idx, batch_size, num_batches,\n",
    "                          batch_remainder):\n",
    "        \"\"\"\n",
    "        Calculates the mini-batch indices in the larger data set\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        curr_idx : the batch index\n",
    "        batch_size : the number of examples in each batch\n",
    "        num_batches : the number of batches to split the total epoch into\n",
    "        batch_remainder : the number of batches that don't fit into\n",
    "                          num_batches groups of size batch_size in the full\n",
    "                          dataset\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a list of indices into the main data set to use in the current batch\n",
    "        \"\"\"\n",
    "\n",
    "        startIdx = curr_idx * batch_size\n",
    "        endIdx = (curr_idx + 1) * batch_size\n",
    "\n",
    "        # may need to make the last batch extra big to handle spillover\n",
    "        makeLastBatchBigger = False\n",
    "        if batch_remainder != 0:\n",
    "            makeLastBatchBigger = True\n",
    "\n",
    "        atLastBatch = curr_idx == (num_batches - 1)\n",
    "        if atLastBatch and makeLastBatchBigger:\n",
    "            endIdx += batch_remainder\n",
    "\n",
    "        batchIndices = range(startIdx, endIdx)\n",
    "\n",
    "        return batchIndices\n",
    "    \n",
    "    def mini_batch_update(self, X, y):\n",
    "        \"\"\"\n",
    "        One epoch of mini-batch SGD over the entire dataset (i.e. one sweep of\n",
    "        the dataset).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NumPy array of features (size : no of examples X features)\n",
    "        y : Numpy array of class labels (size : no of examples X 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss after epoch\n",
    "        \"\"\"\n",
    "\n",
    "        N = len(y)\n",
    "        if self.batch_size > N:\n",
    "            msg = 'batch size: ' + str(self.batch_size) + \\\n",
    "                  ' cannot be larger than' + \\\n",
    "                  ' the number of examples: ' + str(N)\n",
    "            raise ValueError(msg)\n",
    "            \n",
    "        numBatches, remainder = divmod(N, self.batch_size)\n",
    "\n",
    "        for i in range(numBatches):\n",
    "\n",
    "            # need to shuffle the features each epoch for SGD\n",
    "            X, y = shuffle(X, y)\n",
    "            batchIndices = self.get_batch_indices(i, self.batch_size,\n",
    "                                                  numBatches, remainder)\n",
    "            features = X[batchIndices, :]\n",
    "            targets = y[batchIndices]\n",
    "            \n",
    "            # run forward sweep of network to calculate new network\n",
    "            # prediction for loss calculation in grad descent\n",
    "            y_hat = self.forward(X=features)\n",
    "            \n",
    "            # calculate weight gradients using backprop\n",
    "            l1_grad, l2_grad = self.backward(y=targets, y_hat=y_hat)\n",
    "            \n",
    "            # perform the gradient descent update step using the new weight\n",
    "            # matrices and the bias gradients (why are these private class \n",
    "            # instance variables? who knows.)\n",
    "            self.update(l1_grad, l2_grad)\n",
    "\n",
    "        return self.compute_MSE_loss(y, self.forward(X))\n",
    "\n",
    "    def train(self, X, y, maxEpochs, X_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Implement the train loop for n epochs.\n",
    "        In each epoch do the following:\n",
    "        1. Shuffle the dataset.\n",
    "        2. create self.batch_size sized-mini-batches of the dataset.\n",
    "        3. get network predictions using forward().\n",
    "        4. calculate the gradients using backward().\n",
    "        5. update the network weights using update().\n",
    "        6. Repeat 1-5 until convergence.\n",
    "\n",
    "        params:\n",
    "\n",
    "        X (np.ndarray): N x 5 dimensional ndarray of inputs.\n",
    "        y (np.ndarray): N x 1 dimensional array of true labels.\n",
    "        maxEpochs (int): # of epochs to train the network for.\n",
    "        \n",
    "        optional\n",
    "        X_test (np.ndarray): N x 5 dimensional ndarray of test inputs.\n",
    "        y_test (np.ndarray): N x 1 dimensional array of true test labels.\n",
    "\n",
    "        return:\n",
    "\n",
    "        list of final MSE losses after each epoch,\n",
    "        list of final test accuracies after each epoch on the data\n",
    "        (defaults to training accuracy, will use test if present)\n",
    "        \"\"\"\n",
    "        \n",
    "        if (X_test is not None) and (y_test is not None):\n",
    "            computeAccuraciesWithTestData = True\n",
    "        else:\n",
    "            computeAccuraciesWithTestData = False\n",
    "        \n",
    "        # define a loss tolerance as an additional termination condition\n",
    "        lossRelTol = 1e-6\n",
    "        loss = np.inf\n",
    "        lossRelDiff = np.inf\n",
    "\n",
    "        # the only difference between SGD, mini-batch, and full batch grad\n",
    "        # descent is batch sizes\n",
    "        N = len(y)\n",
    "\n",
    "        # initialize gradient descent with weights all being 0\n",
    "        numFeatures = X.shape[1]\n",
    "        w = np.zeros((numFeatures, 1))\n",
    "        numEpochs = 0\n",
    "        lossPerEpoch = []\n",
    "        accuracyPerEpoch = []\n",
    "        while (numEpochs < maxEpochs) and (lossRelDiff > lossRelTol):\n",
    "\n",
    "            newLoss = self.mini_batch_update(X, y)\n",
    "            \n",
    "            # compute new model accuracy on the desired data set\n",
    "            if computeAccuraciesWithTestData:\n",
    "                X_acc = X_test\n",
    "                y_acc = y_test\n",
    "            else:\n",
    "                X_acc = X\n",
    "                y_acc = y\n",
    "\n",
    "            y_hat = self.predict(X_acc)\n",
    "            currAccuracy = self.accuracy(y_acc, y_hat)\n",
    "            accuracyPerEpoch.append(currAccuracy)\n",
    "            \n",
    "            # compute relative loss improvement\n",
    "            lossRelDiff = np.abs(loss - newLoss)\n",
    "            loss = newLoss\n",
    "            lossPerEpoch.append(loss)\n",
    "\n",
    "            # slow down printing of loss\n",
    "            if (numEpochs % 10) == 0:\n",
    "                print('Epoch: ', numEpochs, 'Loss: ', loss)\n",
    "\n",
    "            numEpochs += 1\n",
    "\n",
    "        print('Max Epochs: ', numEpochs,\n",
    "              'Final Loss: ', loss,\n",
    "              'Final Accuracy: ', currAccuracy)\n",
    "\n",
    "        return lossPerEpoch, accuracyPerEpoch\n",
    "            \n",
    "    def test(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Implement the test function which reports accuracy of \n",
    "        the model on the test set X_test against the true labels\n",
    "        y_test.\n",
    "\n",
    "        You may re-use the predict and accuracy functions defined above.\n",
    "\n",
    "        params:\n",
    "\n",
    "        X_test (np.ndarray): N x M dimensional array where M is the \n",
    "            number of attributes considered for training the model.\n",
    "        y_test (np.ndarray): N x 1 dimensional array.\n",
    "\n",
    "        return:\n",
    "\n",
    "        accuracy (double): accuracy of the predicted labels against the \n",
    "            groundtruth labels.\n",
    "        \"\"\"\n",
    "\n",
    "        y_hat = self.predict(X_test)\n",
    "\n",
    "        return self.accuracy(y_test, y_hat)\n",
    "    \n",
    "    @classmethod\n",
    "    def accuracy(self, y, y_hat):\n",
    "        numFeatures = y.flatten().shape[0]\n",
    "        return np.sum(y.flatten() == y_hat.flatten()) / numFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:37:25.886097Z",
     "start_time": "2020-02-19T22:37:22.972939Z"
    },
    "code_folding": [
     3,
     9
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  124.25668927614994\n",
      "Epoch:  10 Loss:  163.94059403718654\n",
      "Epoch:  20 Loss:  185.69492265634656\n",
      "Epoch:  30 Loss:  194.50938824361876\n",
      "Epoch:  40 Loss:  200.816372355032\n",
      "Epoch:  50 Loss:  203.87722061671494\n",
      "Epoch:  60 Loss:  205.96541551539028\n",
      "Epoch:  70 Loss:  207.86155615388662\n",
      "Epoch:  80 Loss:  208.58179585293536\n",
      "Epoch:  90 Loss:  209.66291914113333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Epochs:  100 Final Loss:  210.57521395198535 Final Accuracy:  0.9722222222222222\n",
      "test accuracy: 0.98989898989899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 2.861s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def load_params(path, model):\n",
    "    \n",
    "    params = pickle.load(open(path, 'rb'))\n",
    "    for key in params:\n",
    "        model.__dict__[key] = params[key]\n",
    "\n",
    "class TestMLP(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        \n",
    "        self.net = MLP()\n",
    "        self.pre_trained_model = load_params(\"./res/mlp.pkl\", self.net)\n",
    "        self.X = np.load(\"./res/x.npy\")\n",
    "        self.y = np.load(\"./res/y.npy\")\n",
    "        self.p = np.load(\"./res/p.npy\")\n",
    "        self.sigX = np.asarray([-1, -2, -3, 0, 1, 2, 3])\n",
    "    \n",
    "    def test_forward(self):\n",
    "        self.assertEqual(np.sum(self.net.forward(self.X[901:])),\n",
    "                         np.sum(self.p))\n",
    "    \n",
    "    def test_train(self):\n",
    "        x, y = make_classification(\n",
    "            n_samples=1000,\n",
    "            n_features=5,\n",
    "            n_redundant=0,\n",
    "            random_state=rndg\n",
    "        )\n",
    "        net = MLP(5, 5, 1, 7, 0.01)\n",
    "        net.train(self.X[:900], self.y[:900], 100)\n",
    "        testAccuracy = net.test(self.X[901:], self.y[901:])\n",
    "        print('test accuracy:', testAccuracy)\n",
    "        self.assertTrue(testAccuracy > 0.88)\n",
    "    \n",
    "    def test_sigmoid(self):\n",
    "        self.assertEqual(\n",
    "            np.sum(self.net.sigmoid(self.sigX)), 3.5\n",
    "        )\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestMLP)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2.B**\n",
    "\n",
    "Report the baseline performance of the untrained MLP that you built in **PART 2.A** using the selected values. To calculate the baseline score, feed-forward entire test set through the test function. The test function will return a set of predictions. Calculate the accuracy of these predictions against ground truth training lables.  \n",
    "Report:\n",
    "1. accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:37:25.893255Z",
     "start_time": "2020-02-19T22:37:25.887929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.2282208588957055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-71db5e83db0b>:70: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + numpy.exp(-X))\n"
     ]
    }
   ],
   "source": [
    "classifier = MLP()\n",
    "baselineAccuracy = classifier.test(X_test, y_test)\n",
    "print(\"Baseline accuracy:\", baselineAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2.C**\n",
    "\n",
    "Select a learning rate, batch size and train the network for 100 epochs. Plot a graph of network's accuracy on the test data as a function of epoch. On the same graph, plot a constant horizontal line of the baseline score that you reported in **PART 2.B**.  \n",
    "\n",
    "Report:\n",
    "1. learning rate\n",
    "2. batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:44:06.681281Z",
     "start_time": "2020-02-19T22:43:31.551825Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-71db5e83db0b>:70: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + numpy.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.09091494403281337\n",
      "Epoch:  10 Loss:  0.05559586651787933\n",
      "Epoch:  20 Loss:  0.05457309168636241\n",
      "Epoch:  30 Loss:  0.050886934889714124\n",
      "Epoch:  40 Loss:  0.04897280252384355\n",
      "Epoch:  50 Loss:  0.053442987510561736\n",
      "Epoch:  60 Loss:  0.04813847408723067\n",
      "Epoch:  70 Loss:  0.04667221881818712\n",
      "Epoch:  80 Loss:  0.0447754963760889\n",
      "Epoch:  90 Loss:  0.04545053882181078\n",
      "Epoch:  100 Loss:  0.044850600549507305\n",
      "Epoch:  110 Loss:  0.04522285014536697\n",
      "Epoch:  120 Loss:  0.042830305300392814\n",
      "Epoch:  130 Loss:  0.04235386432467418\n",
      "Epoch:  140 Loss:  0.04168627240357401\n",
      "Epoch:  150 Loss:  0.03958953223922048\n",
      "Epoch:  160 Loss:  0.0389074970020256\n",
      "Epoch:  170 Loss:  0.039071417570832115\n",
      "Epoch:  180 Loss:  0.03843851745254349\n",
      "Epoch:  190 Loss:  0.038113453074655544\n",
      "Max Epochs:  200 Final Loss:  0.037217031597840906 Final Accuracy:  0.90920245398773\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(indices, lossData, xlabel, ylabel, **kwargs):\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    # make it wide to see the timeseries more easily\n",
    "    plt.rcParams['figure.figsize'] = [15, 5]\n",
    "    \n",
    "    if 'baseline' in kwargs:\n",
    "        baseline = kwargs['baseline']\n",
    "        plt.plot(indices, np.ones(len(indices)) * baseline, ':')\n",
    "    \n",
    "    for loss in lossData:\n",
    "        plt.plot(indices, loss, '-')\n",
    "    \n",
    "    if 'title' in kwargs:\n",
    "        plt.title(kwargs['title'])\n",
    "    else:\n",
    "        plt.title('Linear Regression MSE Loss')\n",
    "    plt.xlabel(xlabel);\n",
    "    plt.ylabel(ylabel);\n",
    "    \n",
    "    if 'legendStrings' in kwargs:\n",
    "        plt.legend(kwargs['legendStrings'])\n",
    "\n",
    "lr = 0.00001\n",
    "batch_size = 20\n",
    "classifier = MLP(batch_size=batch_size, lr=lr)\n",
    "maxEpochs = 200\n",
    "loss, accuracyPerEpoch = classifier.train(X_train, y_train,\n",
    "                                          maxEpochs=maxEpochs,\n",
    "                                          X_test=X_test,\n",
    "                                          y_test=y_test)       \n",
    "\n",
    "test_epoch_indices = range(0, len(accuracyPerEpoch))\n",
    "plot_loss(test_epoch_indices, [accuracyPerEpoch[0:maxEpochs]],\n",
    "          xlabel='epoch number',\n",
    "          ylabel='accuracy',\n",
    "          title='Classifier Test Accuracy Evolution',\n",
    "          baseline=baselineAccuracy,\n",
    "          legendStrings=('baseline',\n",
    "                         'trained: lr = %0.3g, batch_size = %d' % (lr,\n",
    "                          batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2.D**\n",
    "\n",
    "Now train nets with varying size of the hidden layer $H={1, 2, 5, 10..}$ for max epochs = 100. Make a plot of the nets' accuracy on test set as a function of $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:40:22.069611Z",
     "start_time": "2020-02-19T22:37:44.038898Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-71db5e83db0b>:70: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + numpy.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.2185865483566485\n",
      "Epoch:  10 Loss:  0.21810079392080664\n",
      "Epoch:  20 Loss:  0.21767546203793797\n",
      "Epoch:  30 Loss:  0.21724958860300456\n",
      "Epoch:  40 Loss:  0.21682241141186945\n",
      "Epoch:  50 Loss:  0.2163992529307883\n",
      "Epoch:  60 Loss:  0.2159741298431692\n",
      "Epoch:  70 Loss:  0.21555035532838027\n",
      "Epoch:  80 Loss:  0.2151272726384951\n",
      "Epoch:  90 Loss:  0.21470278498586243\n",
      "Max Epochs:  100 Final Loss:  0.2143210540917173 Final Accuracy:  0.2105622270742358\n",
      "Test set accuracy for n_hid=1: 0.22822\n",
      "Epoch:  0 Loss:  0.21852091092025708\n",
      "Epoch:  10 Loss:  0.21809541131552887\n",
      "Epoch:  20 Loss:  0.21766729328902126\n",
      "Epoch:  30 Loss:  0.21724300468559396\n",
      "Epoch:  40 Loss:  0.21681803745725745\n",
      "Epoch:  50 Loss:  0.21639309583554356\n",
      "Epoch:  60 Loss:  0.21596738252618217\n",
      "Epoch:  70 Loss:  0.21554165599938885\n",
      "Epoch:  80 Loss:  0.21511597508030397\n",
      "Epoch:  90 Loss:  0.21469153857849482\n",
      "Max Epochs:  100 Final Loss:  0.2143120126292842 Final Accuracy:  0.2105622270742358\n",
      "Test set accuracy for n_hid=2: 0.22822\n",
      "Epoch:  0 Loss:  0.04951206697855075\n",
      "Epoch:  10 Loss:  0.030704674062538878\n",
      "Epoch:  20 Loss:  0.03419852021988179\n",
      "Epoch:  30 Loss:  0.036820891421567435\n",
      "Epoch:  40 Loss:  0.03957317757758634\n",
      "Epoch:  50 Loss:  0.04148138804695206\n",
      "Epoch:  60 Loss:  0.042441831588609646\n",
      "Epoch:  70 Loss:  0.04232382465283745\n",
      "Epoch:  80 Loss:  0.043490692967545985\n",
      "Epoch:  90 Loss:  0.04475415761876362\n",
      "Max Epochs:  100 Final Loss:  0.0453118862043037 Final Accuracy:  0.8590338427947598\n",
      "Test set accuracy for n_hid=5: 0.85031\n",
      "Epoch:  0 Loss:  0.08453824975100206\n",
      "Epoch:  10 Loss:  0.04234264493931339\n",
      "Epoch:  20 Loss:  0.05151114752357362\n",
      "Epoch:  30 Loss:  0.05369709559109779\n",
      "Epoch:  40 Loss:  0.056372642118430794\n",
      "Epoch:  50 Loss:  0.05818363332174292\n",
      "Epoch:  60 Loss:  0.04986265730439003\n",
      "Epoch:  70 Loss:  0.0489834575139333\n",
      "Epoch:  80 Loss:  0.04824512754406445\n",
      "Epoch:  90 Loss:  0.046525014095511755\n",
      "Max Epochs:  100 Final Loss:  0.04774361568914222 Final Accuracy:  0.896424672489083\n",
      "Test set accuracy for n_hid=10: 0.88344\n",
      "Epoch:  0 Loss:  0.12280986013496349\n",
      "Epoch:  10 Loss:  0.04563186582577343\n",
      "Epoch:  20 Loss:  0.0461140259665054\n",
      "Epoch:  30 Loss:  0.04772229145544141\n",
      "Epoch:  40 Loss:  0.04646245589174072\n",
      "Epoch:  50 Loss:  0.042998426130493046\n",
      "Epoch:  60 Loss:  0.04047083033020177\n",
      "Epoch:  70 Loss:  0.04001290391908355\n",
      "Epoch:  80 Loss:  0.037985594540453096\n",
      "Epoch:  90 Loss:  0.03667722273864679\n",
      "Max Epochs:  100 Final Loss:  0.03673938033050551 Final Accuracy:  0.9205786026200873\n",
      "Test set accuracy for n_hid=15: 0.9092\n",
      "Epoch:  0 Loss:  0.07846628060940777\n",
      "Epoch:  10 Loss:  0.0524752627026985\n",
      "Epoch:  20 Loss:  0.03786926670732488\n",
      "Epoch:  30 Loss:  0.03571509631332716\n",
      "Epoch:  40 Loss:  0.03489297941788969\n",
      "Epoch:  50 Loss:  0.03443224973448487\n",
      "Epoch:  60 Loss:  0.03142449208509396\n",
      "Epoch:  70 Loss:  0.030212464167023508\n",
      "Epoch:  80 Loss:  0.030974812209182273\n",
      "Epoch:  90 Loss:  0.032291704776171375\n",
      "Max Epochs:  100 Final Loss:  0.03269374629606147 Final Accuracy:  0.9243995633187773\n",
      "Test set accuracy for n_hid=25: 0.91534\n",
      "Epoch:  0 Loss:  0.1509683094117516\n",
      "Epoch:  10 Loss:  0.029232323421151746\n",
      "Epoch:  20 Loss:  0.02854601081039342\n",
      "Epoch:  30 Loss:  0.026801640494146663\n",
      "Epoch:  40 Loss:  0.026383850017225926\n",
      "Epoch:  50 Loss:  0.02665073826269438\n",
      "Epoch:  60 Loss:  0.027383211479505277\n",
      "Epoch:  70 Loss:  0.027957365167952414\n",
      "Epoch:  80 Loss:  0.028141189309761637\n",
      "Epoch:  90 Loss:  0.028849080284407906\n",
      "Max Epochs:  96 Final Loss:  0.028870942224805893 Final Accuracy:  0.9319050218340611\n",
      "Test set accuracy for n_hid=40: 0.92638\n",
      "Epoch:  0 Loss:  0.08736701387393934\n",
      "Epoch:  10 Loss:  0.043047726029908764\n",
      "Epoch:  20 Loss:  0.04032578080410714\n",
      "Epoch:  30 Loss:  0.04273375509897247\n",
      "Epoch:  40 Loss:  0.045086389575166974\n",
      "Epoch:  50 Loss:  0.04279022082345981\n",
      "Epoch:  60 Loss:  0.04013876996107587\n",
      "Epoch:  70 Loss:  0.037752381448826575\n",
      "Epoch:  80 Loss:  0.03349577998094577\n",
      "Epoch:  90 Loss:  0.031379157621597396\n",
      "Max Epochs:  100 Final Loss:  0.03173018008440331 Final Accuracy:  0.9329967248908297\n",
      "Test set accuracy for n_hid=50: 0.92761\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_layer_sizes = [1, 2, 5, 10, 15, 25, 40, 50]\n",
    "lr = 0.00001\n",
    "batch_size = 20\n",
    "maxEpochs = 100\n",
    "\n",
    "accuracies = []\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    \n",
    "    classifier = MLP(n_hid=hidden_layer_size, batch_size=batch_size, lr=lr)\n",
    "    classifier.train(X_train, y_train, maxEpochs=maxEpochs)\n",
    "    \n",
    "    currAccuracy = classifier.test(X_test, y_test)\n",
    "    accuracies.append(currAccuracy)\n",
    "    \n",
    "    print('Test set accuracy for n_hid=%d: %0.5g' %\n",
    "          (hidden_layer_size, currAccuracy))\n",
    "\n",
    "plot_loss(hidden_layer_sizes, [accuracies],\n",
    "          xlabel='hidden layer size',\n",
    "          ylabel='test set accuracy',\n",
    "          title='Accuracy Variation with Number of Hidden Layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much adding information about time of day helps the network. Add a new set of inputs that represent the time of day. (Don't add information about day of week or absolute date.)  \n",
    "\n",
    "\n",
    "**PART 3.A**  \n",
    "\n",
    "Determine an appropriate representation for the time of day. Describe the representation you used. For example, you might add one unit with a value ranging from 0 to 1 for times ranging from 00:00 to 23:59. Report the representation you selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:40:22.073399Z",
     "start_time": "2020-02-19T22:40:22.071374Z"
    }
   },
   "outputs": [],
   "source": [
    "### Part 3.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 3.B**  \n",
    "\n",
    "Train your net with $H=5$ hidden and compare training and test set performance to the net you built in (2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T22:40:22.076992Z",
     "start_time": "2020-02-19T22:40:22.075166Z"
    }
   },
   "outputs": [],
   "source": [
    "### Part 3.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
