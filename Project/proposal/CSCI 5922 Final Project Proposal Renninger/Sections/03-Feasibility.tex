As we are most interested in comparing the structure, parameters, and language distribution of the extracted state machine model with that of a more traditional state machine learning algorithm, the training / testing data is most usefully generated at will from a target state machine (this is very common in any study done on a state machine learning algorithm). Besides generating training / test data from desired language model state machine targets, I can also utilize widely available language modeling competition datasets (i.e. PAutomaC \cite{Verwer_PAutomaC}).

This project will focus on the ability to extract a state machine from a DNN language model which means there is actually a quite straightforward way to evaluate the feasibility of this sort of technique in general. I can not only evaluate standard language model measures like perplexity score \cite{Verwer_PAutomaC}, but also information theoretic measures of model performance like coding rate, model \href{https://weberna.github.io/jekyll/update/2017/11/08/Information-Bottleneck-Part1.html#fnref:3}{distortion} (e.g. \href{https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8}{Kullback-Leibler (KL) Divergence} or basic predictive accuracy), and \href{https://weberna.github.io/jekyll/update/2017/11/08/Information-Bottleneck-Part1.html#fnref:3}{mutual information} \cite{Marzen2019}. These comparisons will allow the information theoretic properties of traditionally inferred probabilistic state machines to be readily compared to that those of learned DNNs and those of state machines extracted from these same learned DNNs.

In the case that the DNN is learned through a deep (possibly inverse) RL algorithm, there will obviously be ample data to train the DNN policy / value model and well studied reward evaluation models from the field of uncertain decision-making; thus the problem is still feasible from this RL angle. 

As outlined in Section \ref{sec: approaches}, the methods I will be trying out are not incredibly computationally intensive, as far as deep learning goes.

Thus, there is ample evidence to suggest that there is enough data to learn and evaluate the proposed models, so this project does not have any immediately obvious feasibility problems.